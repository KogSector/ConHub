# Enhanced Indexing System

A comprehensive, production-ready indexing system with advanced features including real-time updates, multi-format processing, embedding generation, error handling, and schema evolution.

## üöÄ Features

### Core Capabilities
- **Real-time Indexing**: Live file system monitoring with debounced updates
- **Multi-source Support**: PostgreSQL, S3/SQS, file system, and more
- **Advanced Pattern Matching**: Glob, regex, MIME type, and content-based filtering
- **Multi-format Processing**: PDF, images, documents, archives, and web content
- **Embedding Generation**: Text, image, multimodal (ColPali), document, and code embeddings
- **Schema Evolution**: Automatic migration with compatibility checking
- **Comprehensive Error Handling**: Retry logic, circuit breakers, and recovery strategies
- **Performance Monitoring**: Real-time metrics, alerting, and performance optimization

### Advanced Features
- **Incremental Indexing**: Process only changed content with ordinal tracking
- **Batch Processing**: Efficient handling of large datasets
- **Caching**: Multi-level caching for improved performance
- **Compression**: Automatic content compression
- **Deduplication**: Content-based duplicate detection
- **Health Monitoring**: Circuit breakers and auto-recovery

## üìã Table of Contents

1. [Quick Start](#quick-start)
2. [Architecture](#architecture)
3. [Configuration](#configuration)
4. [Sources](#sources)
5. [Transforms](#transforms)
6. [Error Handling](#error-handling)
7. [Monitoring](#monitoring)
8. [Schema Evolution](#schema-evolution)
9. [Performance Tuning](#performance-tuning)
10. [Examples](#examples)
11. [API Reference](#api-reference)
12. [Contributing](#contributing)

## üöÄ Quick Start

### Installation

Add to your `Cargo.toml`:

```toml
[dependencies]
enhanced-indexers = "0.1.0"
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
```

### Basic Example

```rust
use enhanced_indexers::prelude::*;
use serde_json::json;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configure the indexing pipeline
    let config = json!({
        "sources": [{
            "type": "file_system",
            "path": "./documents",
            "pattern": "**/*.{pdf,txt,md}",
            "recursive": true
        }],
        "transforms": [
            {
                "type": "multi_format_processor",
                "config": {
                    "enable_text_extraction": true,
                    "enable_metadata_extraction": true,
                    "pdf_config": {
                        "extract_images": true,
                        "ocr_enabled": true
                    }
                }
            },
            {
                "type": "embedding_processor",
                "config": {
                    "text_embedding": {
                        "model": "sentence-transformers/all-MiniLM-L6-v2",
                        "chunk_size": 512,
                        "overlap": 50
                    }
                }
            }
        ],
        "sinks": [{
            "type": "elasticsearch",
            "url": "http://localhost:9200",
            "index": "documents"
        }],
        "error_handling": {
            "retry_config": {
                "max_retries": 3,
                "backoff_strategy": "exponential"
            }
        },
        "monitoring": {
            "enable_metrics": true,
            "export_targets": ["prometheus"]
        }
    });

    // Create and run the indexer
    let indexer = IndexerBuilder::from_config(config)?
        .build()
        .await?;

    let results = indexer.run().await?;
    
    println!("Indexed {} documents", results.processed_count);
    println!("Generated {} embeddings", results.embeddings_count);
    
    Ok(())
}
```

## üèóÔ∏è Architecture

The enhanced indexing system follows a modular, pipeline-based architecture:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Sources   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Transforms  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Sinks    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Monitoring  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                   ‚îÇ                   ‚îÇ                   ‚îÇ
       ‚ñº                   ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Live Updates‚îÇ    ‚îÇ Error Handler‚îÇ    ‚îÇSchema Evol. ‚îÇ    ‚îÇ   Metrics    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components

- **Sources**: Data input from various systems (file system, databases, cloud storage)
- **Transforms**: Data processing and enrichment (format conversion, embedding generation)
- **Sinks**: Data output to target systems (databases, search engines, files)
- **Live Updates**: Real-time monitoring and incremental processing
- **Error Handler**: Comprehensive error handling with retry logic and circuit breakers
- **Schema Evolution**: Automatic schema migration and compatibility checking
- **Monitoring**: Performance metrics, alerting, and health monitoring

## ‚öôÔ∏è Configuration

### Configuration Structure

```json
{
  "sources": [...],
  "transforms": [...],
  "sinks": [...],
  "live_updates": {...},
  "error_handling": {...},
  "monitoring": {...},
  "schema_evolution": {...},
  "performance": {...}
}
```

### Environment Variables

```bash
# Database connections
POSTGRES_URL=postgresql://user:pass@localhost/db
ELASTICSEARCH_URL=http://localhost:9200

# Cloud services
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1

# Embedding models
OPENAI_API_KEY=your_openai_key
HUGGINGFACE_API_KEY=your_hf_key

# Monitoring
PROMETHEUS_ENDPOINT=http://localhost:9090
GRAFANA_URL=http://localhost:3000
```

## üì• Sources

### File System Source

```json
{
  "type": "file_system",
  "path": "./documents",
  "pattern": "**/*.{pdf,txt,md,docx}",
  "recursive": true,
  "watch_for_changes": true,
  "filters": {
    "mime_types": ["application/pdf", "text/plain"],
    "size_range": {"min": 1024, "max": 10485760},
    "modified_after": "2024-01-01T00:00:00Z"
  }
}
```

### PostgreSQL Source

```json
{
  "type": "postgres",
  "connection_string": "postgresql://user:pass@localhost/db",
  "incremental_config": {
    "enable": true,
    "ordinal_column": "updated_at",
    "batch_size": 1000,
    "state_storage": {
      "type": "database",
      "table": "indexing_state"
    }
  },
  "notification_config": {
    "enable": true,
    "channels": ["document_updates", "schema_changes"],
    "debounce_duration": 5000
  },
  "pool_config": {
    "max_connections": 10,
    "min_connections": 2,
    "connection_timeout": 30
  }
}
```

### Amazon S3 Source (with optional SQS change stream)

```json
{
  "type": "AmazonS3",
  "aws_config": {
    "region": "us-east-1",
    "access_key_id": "${AWS_ACCESS_KEY_ID}",
    "secret_access_key": "${AWS_SECRET_ACCESS_KEY}"
  },
  "bucket_name": "my-documents",
  "prefix": "documents/",
  "binary": false,
  "included_patterns": ["*.pdf", "*.txt"],
  "excluded_patterns": ["temp/*", "*.tmp"],
  "sqs_queue_url": "https://sqs.us-east-1.amazonaws.com/123456789/document-updates"
}
```

## üîÑ Transforms

### Multi-Format Processor

```json
{
  "type": "multi_format_processor",
  "config": {
    "pdf_config": {
      "extract_text": true,
      "extract_images": true,
      "extract_metadata": true,
      "ocr_config": {
        "engine": "tesseract",
        "languages": ["eng", "spa"],
        "confidence_threshold": 0.8
      },
      "image_processing": {
        "resize": {"max_width": 1920, "max_height": 1080},
        "format": "jpeg",
        "quality": 85
      }
    },
    "image_config": {
      "extract_metadata": true,
      "generate_thumbnails": true,
      "ocr_enabled": true,
      "feature_extraction": {
        "enable": true,
        "model": "resnet50"
      }
    },
    "document_config": {
      "extract_text": true,
      "preserve_formatting": true,
      "extract_tables": true,
      "extract_headers": true
    },
    "caching": {
      "enable": true,
      "cache_size": 1000,
      "ttl": 3600
    }
  }
}
```

### Embedding Processor

```json
{
  "type": "embedding_processor",
  "config": {
    "text_embedding": {
      "model_type": "sentence_transformers",
      "model_name": "all-MiniLM-L6-v2",
      "chunk_config": {
        "strategy": "semantic",
        "chunk_size": 512,
        "overlap": 50,
        "min_chunk_size": 100
      },
      "preprocessing": {
        "normalize_text": true,
        "remove_special_chars": false,
        "language_detection": true
      }
    },
    "image_embedding": {
      "model_type": "clip",
      "model_name": "openai/clip-vit-base-patch32",
      "preprocessing": {
        "resize": {"width": 224, "height": 224},
        "normalize": true,
        "augmentation": false
      }
    },
    "multimodal_embedding": {
      "model_type": "colpali",
      "model_name": "vidore/colpali",
      "config": {
        "vision_encoder": "google/paligemma-3b-pt-224",
        "text_encoder": "google/paligemma-3b-pt-224",
        "max_length": 512,
        "vision_config": {
          "image_size": 224,
          "patch_size": 16
        }
      }
    },
    "batch_config": {
      "batch_size": 32,
      "max_batch_wait_time": 1000,
      "parallel_batches": 2
    },
    "caching": {
      "enable": true,
      "cache_type": "redis",
      "redis_url": "redis://localhost:6379",
      "ttl": 86400
    }
  }
}
```

## üõ°Ô∏è Error Handling

### Comprehensive Error Configuration

```json
{
  "error_handling": {
    "retry_config": {
      "max_retries": 3,
      "backoff_strategy": "exponential",
      "initial_delay": 1000,
      "max_delay": 30000,
      "jitter": true,
      "retry_on": ["timeout", "connection_error", "rate_limit"]
    },
    "circuit_breaker": {
      "failure_threshold": 5,
      "success_threshold": 3,
      "timeout": 60000,
      "half_open_max_calls": 3,
      "listeners": ["log", "metrics", "alert"]
    },
    "recovery": {
      "enable_auto_recovery": true,
      "health_check_interval": 30000,
      "recovery_strategies": ["restart", "fallback", "skip"],
      "graceful_degradation": {
        "enable": true,
        "fallback_processing": true,
        "reduced_quality": true
      }
    },
    "reporting": {
      "targets": ["log", "metrics", "webhook"],
      "webhook_url": "https://api.example.com/errors",
      "aggregation": {
        "window_size": 300000,
        "threshold": 10
      }
    },
    "dlq": {
      "enable": true,
      "storage_type": "file",
      "file_config": {
        "path": "./failed_items",
        "max_size": 1000,
        "retention_days": 7
      }
    }
  }
}
```

## üìä Monitoring

### Metrics Configuration

```json
{
  "monitoring": {
    "metrics": {
      "enable": true,
      "collection_interval": 10000,
      "categories": ["performance", "health", "business", "system"],
      "custom_metrics": {
        "document_processing_rate": {
          "type": "gauge",
          "description": "Documents processed per second"
        },
        "embedding_generation_latency": {
          "type": "histogram",
          "description": "Time to generate embeddings",
          "buckets": [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
        }
      }
    },
    "storage": {
      "type": "prometheus",
      "prometheus_config": {
        "endpoint": "http://localhost:9090",
        "job_name": "enhanced-indexer",
        "push_interval": 15000
      }
    },
    "export": {
      "targets": ["prometheus", "grafana", "cloudwatch"],
      "prometheus": {
        "endpoint": "http://localhost:9090",
        "job_name": "indexer"
      },
      "grafana": {
        "url": "http://localhost:3000",
        "dashboard_id": "indexer-dashboard"
      }
    },
    "alerting": {
      "enable": true,
      "rules": [
        {
          "name": "high_error_rate",
          "condition": "error_rate > 0.05",
          "severity": "warning",
          "actions": ["log", "webhook"]
        },
        {
          "name": "processing_latency",
          "condition": "avg_processing_time > 10000",
          "severity": "critical",
          "actions": ["log", "webhook", "page"]
        }
      ]
    }
  }
}
```

## üîÑ Schema Evolution

### Schema Evolution Configuration

```json
{
  "schema_evolution": {
    "enable": true,
    "auto_migration": {
      "enable": true,
      "compatibility_check": true,
      "backup_before_migration": true,
      "rollback_on_failure": true
    },
    "compatibility": {
      "strategy": "backward_compatible",
      "breaking_change_policy": "manual_approval",
      "version_retention": 5
    },
    "versioning": {
      "strategy": "semantic",
      "auto_increment": true,
      "metadata_storage": {
        "type": "database",
        "table": "schema_versions"
      }
    },
    "migration": {
      "batch_size": 1000,
      "parallel_workers": 4,
      "timeout": 300000,
      "strategies": ["in_place", "dual_write", "copy_and_swap"]
    },
    "validation": {
      "enable": true,
      "strict_mode": false,
      "custom_validators": ["business_rules", "data_quality"]
    },
    "notification": {
      "targets": ["webhook", "email"],
      "webhook_url": "https://api.example.com/schema-changes"
    }
  }
}
```

## ‚ö° Performance Tuning

### Performance Configuration

```json
{
  "performance": {
    "concurrency": {
      "max_workers": 8,
      "worker_queue_size": 1000,
      "work_stealing": true
    },
    "caching": {
      "enable": true,
      "levels": ["memory", "disk", "distributed"],
      "memory_cache": {
        "size": "1GB",
        "eviction_policy": "lru"
      },
      "disk_cache": {
        "path": "./cache",
        "size": "10GB",
        "compression": true
      }
    },
    "compression": {
      "enable": true,
      "algorithm": "zstd",
      "level": 3,
      "min_size": 1024
    },
    "connection_pooling": {
      "max_connections": 20,
      "min_connections": 5,
      "idle_timeout": 300000,
      "max_lifetime": 1800000
    },
    "batch_processing": {
      "enable": true,
      "batch_size": 100,
      "max_wait_time": 5000,
      "adaptive_batching": true
    }
  }
}
```

## üìö Examples

### Example 1: Document Processing Pipeline

```rust
use enhanced_indexers::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = IndexerConfig::builder()
        .add_source(FileSystemSource::builder()
            .path("./documents")
            .pattern("**/*.{pdf,docx,txt}")
            .recursive(true)
            .build())
        .add_transform(MultiFormatProcessor::builder()
            .enable_text_extraction(true)
            .enable_metadata_extraction(true)
            .pdf_config(PdfConfig::builder()
                .extract_images(true)
                .ocr_enabled(true)
                .build())
            .build())
        .add_transform(EmbeddingProcessor::builder()
            .text_embedding(TextEmbeddingConfig::builder()
                .model("sentence-transformers/all-MiniLM-L6-v2")
                .chunk_size(512)
                .build())
            .build())
        .add_sink(ElasticsearchSink::builder()
            .url("http://localhost:9200")
            .index("documents")
            .build())
        .error_handling(ErrorHandlingConfig::builder()
            .retry_config(RetryConfig::exponential_backoff(3))
            .circuit_breaker(CircuitBreakerConfig::default())
            .build())
        .monitoring(MonitoringConfig::builder()
            .enable_metrics(true)
            .prometheus_endpoint("http://localhost:9090")
            .build())
        .build();

    let indexer = Indexer::new(config).await?;
    let results = indexer.run().await?;
    
    println!("Processing complete!");
    println!("Documents processed: {}", results.documents_processed);
    println!("Embeddings generated: {}", results.embeddings_generated);
    println!("Errors: {}", results.errors.len());
    
    Ok(())
}
```

### Example 2: Real-time S3 Indexing

```rust
use enhanced_indexers::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = IndexerConfig::builder()
        .add_source(AmazonS3Source::builder()
            .bucket_name("my-documents")
            .prefix("documents/")
            .binary(false)
            .sqs_queue_url("https://sqs.us-east-1.amazonaws.com/123456789/document-updates")
            .build())
        .add_transform(MultiFormatProcessor::default())
        .add_transform(EmbeddingProcessor::builder()
            .multimodal_embedding(MultimodalEmbeddingConfig::builder()
                .model_type(MultimodalModelType::ColPali)
                .model_name("vidore/colpali")
                .build())
            .build())
        .live_updates(LiveUpdateConfig::builder()
            .enable(true)
            .debounce_duration(Duration::from_secs(5))
            .batch_size(10)
            .build())
        .build();

    let indexer = Indexer::new(config).await?;
    
    // Start real-time processing
    indexer.start_live_processing().await?;
    
    // Keep running
    tokio::signal::ctrl_c().await?;
    
    indexer.stop().await?;
    
    Ok(())
}
```

### Example 3: PostgreSQL Incremental Indexing

```rust
use enhanced_indexers::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = IndexerConfig::builder()
        .add_source(PostgresSource::builder()
            .connection_string("postgresql://user:pass@localhost/db")
            .table("documents")
            // Configure `ordinal_column` and optional `notification` in source spec
            .build())
        .add_transform(EmbeddingProcessor::builder()
            .text_embedding(TextEmbeddingConfig::builder()
                .model("sentence-transformers/all-MiniLM-L6-v2")
                .build())
            .build())
        .schema_evolution(SchemaEvolutionConfig::builder()
            .enable(true)
            .auto_migration(true)
            .compatibility_check(true)
            .build())
        .build();

    let indexer = Indexer::new(config).await?;
    
    // Run incremental indexing
    loop {
        let results = indexer.run_incremental().await?;
        
        if results.processed_count > 0 {
            println!("Processed {} new/updated documents", results.processed_count);
        }
        
        tokio::time::sleep(Duration::from_secs(30)).await;
    }
}
```

## üìñ API Reference

### Core Types

```rust
// Main indexer interface
pub struct Indexer {
    config: IndexerConfig,
    sources: Vec<Box<dyn Source>>,
    transforms: Vec<Box<dyn Transform>>,
    sinks: Vec<Box<dyn Sink>>,
    error_handler: ErrorHandler,
    metrics_collector: MetricsCollector,
}

impl Indexer {
    pub async fn new(config: IndexerConfig) -> Result<Self, IndexerError>;
    pub async fn run(&self) -> Result<IndexingResults, IndexerError>;
    pub async fn run_incremental(&self) -> Result<IncrementalResults, IndexerError>;
    pub async fn start_live_processing(&self) -> Result<(), IndexerError>;
    pub async fn stop(&self) -> Result<(), IndexerError>;
}

// Configuration builder
pub struct IndexerConfig {
    pub sources: Vec<SourceConfig>,
    pub transforms: Vec<TransformConfig>,
    pub sinks: Vec<SinkConfig>,
    pub error_handling: ErrorHandlingConfig,
    pub monitoring: MonitoringConfig,
    pub schema_evolution: SchemaEvolutionConfig,
    pub performance: PerformanceConfig,
}

// Results
pub struct IndexingResults {
    pub documents_processed: usize,
    pub embeddings_generated: usize,
    pub processing_duration: Duration,
    pub errors: Vec<IndexingError>,
    pub metrics: HashMap<String, f64>,
}
```

### Source Traits

```rust
#[async_trait]
pub trait Source: Send + Sync {
    async fn initialize(&mut self) -> Result<(), SourceError>;
    async fn fetch_batch(&mut self, batch_size: usize) -> Result<Vec<Document>, SourceError>;
    async fn supports_incremental(&self) -> bool;
    async fn get_incremental_state(&self) -> Result<Option<IncrementalState>, SourceError>;
    async fn set_incremental_state(&mut self, state: IncrementalState) -> Result<(), SourceError>;
}

#[async_trait]
pub trait LiveSource: Source {
    async fn start_live_monitoring(&mut self) -> Result<(), SourceError>;
    async fn stop_live_monitoring(&mut self) -> Result<(), SourceError>;
    async fn get_live_updates(&mut self) -> Result<Vec<Document>, SourceError>;
}
```

### Transform Traits

```rust
#[async_trait]
pub trait Transform: Send + Sync {
    async fn initialize(&mut self) -> Result<(), TransformError>;
    async fn process(&self, document: Document) -> Result<Document, TransformError>;
    async fn process_batch(&self, documents: Vec<Document>) -> Result<Vec<Document>, TransformError>;
    async fn supports_streaming(&self) -> bool;
}

#[async_trait]
pub trait EmbeddingTransform: Transform {
    async fn generate_embeddings(&self, content: &str) -> Result<Vec<f32>, TransformError>;
    async fn generate_image_embeddings(&self, image: &[u8]) -> Result<Vec<f32>, TransformError>;
    async fn generate_multimodal_embeddings(&self, text: &str, image: &[u8]) -> Result<Vec<f32>, TransformError>;
}
```

## üß™ Testing

### Running Tests

```bash
# Run all tests
cargo test

# Run integration tests (requires test environment)
TEST_POSTGRES_URL=postgresql://test:test@localhost/test_db \
TEST_S3_BUCKET=test-bucket \
ENABLE_PERFORMANCE_TESTS=1 \
cargo test --test integration

# Run specific test suites
cargo test --test end_to_end_tests
cargo test --test error_handling_tests
cargo test --test performance_tests
```

### Test Environment Setup

```bash
# Start test services with Docker Compose
docker-compose -f docker-compose.test.yml up -d

# Set environment variables
export TEST_POSTGRES_URL=postgresql://test:test@localhost:5432/test_db
export TEST_S3_BUCKET=test-bucket
export TEST_REDIS_URL=redis://localhost:6379
export ENABLE_PERFORMANCE_TESTS=1
```

## üöÄ Deployment

### Docker Deployment

```dockerfile
FROM rust:1.70 as builder

WORKDIR /app
COPY . .
RUN cargo build --release

FROM debian:bookworm-slim

RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /app/target/release/enhanced-indexer /usr/local/bin/

EXPOSE 8080
CMD ["enhanced-indexer"]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: enhanced-indexer
spec:
  replicas: 3
  selector:
    matchLabels:
      app: enhanced-indexer
  template:
    metadata:
      labels:
        app: enhanced-indexer
    spec:
      containers:
      - name: enhanced-indexer
        image: enhanced-indexer:latest
        ports:
        - containerPort: 8080
        env:
        - name: POSTGRES_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: ELASTICSEARCH_URL
          value: "http://elasticsearch:9200"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

## üìà Performance Benchmarks

### Throughput Benchmarks

| Document Type | Size | Processing Rate | Memory Usage | CPU Usage |
|---------------|------|-----------------|--------------|-----------|
| Plain Text    | 1KB  | 10,000 docs/sec | 50MB        | 20%       |
| PDF           | 1MB  | 100 docs/sec    | 200MB       | 60%       |
| Images        | 2MB  | 50 docs/sec     | 300MB       | 70%       |
| Mixed Content | Var  | 500 docs/sec    | 150MB       | 45%       |

### Embedding Generation Performance

| Model Type | Embedding Dim | Generation Rate | Memory Usage |
|------------|---------------|-----------------|--------------|
| MiniLM-L6  | 384          | 1,000 texts/sec | 500MB       |
| CLIP       | 512          | 200 images/sec  | 1GB         |
| ColPali    | 128          | 50 docs/sec     | 2GB         |

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone the repository
git clone https://github.com/your-org/enhanced-indexers.git
cd enhanced-indexers

# Install dependencies
cargo build

# Run tests
cargo test

# Run with example configuration
cargo run --example basic_indexing
```

### Code Style

We use `rustfmt` and `clippy` for code formatting and linting:

```bash
cargo fmt
cargo clippy -- -D warnings
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üÜò Support

- üìñ [Documentation](https://docs.example.com/enhanced-indexers)
- üí¨ [Discord Community](https://discord.gg/enhanced-indexers)
- üêõ [Issue Tracker](https://github.com/your-org/enhanced-indexers/issues)
- üìß [Email Support](mailto:support@example.com)

## üó∫Ô∏è Roadmap

### Version 0.2.0
- [ ] GraphQL API support
- [ ] Vector database integrations (Pinecone, Weaviate)
- [ ] Advanced NLP transformations
- [ ] Distributed processing support

### Version 0.3.0
- [ ] Machine learning model training integration
- [ ] Advanced analytics and insights
- [ ] Multi-tenant support
- [ ] Enterprise security features

### Version 1.0.0
- [ ] Production-ready stability
- [ ] Comprehensive documentation
- [ ] Enterprise support
- [ ] Performance optimizations

---

**Built with ‚ù§Ô∏è by the Enhanced Indexers Team**