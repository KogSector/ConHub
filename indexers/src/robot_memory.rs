//! Robot Memory Indexer
//! 
//! This module provides Kafka consumers that:
//! - Consume robot episode and semantic event topics
//! - Convert messages into ConHub documents
//! - Index them into vector_rag and graph_rag
//! 
//! This is the bridge between Apache Kafka/Flink and ConHub's knowledge layer.

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{info, warn, error};
use uuid::Uuid;

// ============================================================================
// KAFKA MESSAGE TYPES (consumed from Flink output topics)
// ============================================================================

/// Episode message from Kafka (output from Flink RobotEpisodeBuilderJob)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EpisodeMessage {
    pub robot_id: Uuid,
    pub tenant_id: Uuid,
    pub episode_number: i64,
    pub episode_type: String,
    pub started_at: DateTime<Utc>,
    pub ended_at: DateTime<Utc>,
    pub duration_ms: i64,
    
    /// Natural language summary generated by Flink
    pub summary: String,
    pub detailed_description: Option<String>,
    
    /// Location context
    pub location_id: Option<String>,
    pub location_name: Option<String>,
    pub location_coordinates: Option<LocationCoordinates>,
    
    /// Entities
    pub objects_seen: Vec<String>,
    pub people_involved: Vec<String>,
    pub tasks_related: Vec<String>,
    
    /// Counts
    pub observations_count: i32,
    pub actions_count: i32,
    
    /// Outcome
    pub outcome: Option<EpisodeOutcome>,
    
    /// Quality
    pub confidence_score: Option<f64>,
}

/// Location coordinates
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationCoordinates {
    pub x: f64,
    pub y: f64,
    pub z: Option<f64>,
    pub frame: String,
}

/// Episode outcome
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EpisodeOutcome {
    pub success: bool,
    pub result_type: Option<String>,
    pub error_message: Option<String>,
}

/// Semantic event message from Kafka (output from Flink RobotSemanticEventJob)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SemanticEventMessage {
    pub robot_id: Uuid,
    pub tenant_id: Uuid,
    pub event_type: String,
    pub subject: String,
    pub predicate: String,
    pub object_value: Option<String>,
    
    /// Natural language representation
    pub natural_language: String,
    
    pub confidence: f64,
    pub source_episode_id: Option<Uuid>,
    pub timestamp: DateTime<Utc>,
}

// ============================================================================
// DOCUMENT TYPES (for ConHub knowledge layer)
// ============================================================================

/// Document ready for chunking and indexing
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RobotMemoryDocument {
    pub id: Uuid,
    pub robot_id: Uuid,
    pub tenant_id: Uuid,
    pub document_type: RobotDocumentType,
    
    /// The main text content for embedding
    pub content: String,
    
    /// Structured metadata for filtering
    pub metadata: RobotDocumentMetadata,
    
    pub created_at: DateTime<Utc>,
}

/// Type of robot memory document
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
pub enum RobotDocumentType {
    Episode,
    SemanticFact,
    Observation,
    Action,
}

/// Metadata for robot documents
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RobotDocumentMetadata {
    pub robot_id: String,
    pub tenant_id: String,
    pub document_type: String,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub episode_number: Option<i64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub episode_type: Option<String>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub location_id: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub location_name: Option<String>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub started_at: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub ended_at: Option<String>,
    
    #[serde(default)]
    pub objects: Vec<String>,
    #[serde(default)]
    pub people: Vec<String>,
    #[serde(default)]
    pub tasks: Vec<String>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub confidence_score: Option<f64>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub fact_type: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub subject: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub predicate: Option<String>,
}

// ============================================================================
// GRAPH TYPES (for graph_rag)
// ============================================================================

/// Node types for the robot knowledge graph
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
pub enum GraphNodeType {
    Robot,
    Episode,
    Location,
    Object,
    Person,
    Task,
    SemanticFact,
}

/// A node in the robot knowledge graph
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphNode {
    pub id: String,
    pub node_type: GraphNodeType,
    pub name: String,
    pub properties: HashMap<String, serde_json::Value>,
}

/// Edge types for the robot knowledge graph
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
pub enum GraphEdgeType {
    RobotHadEpisode,
    EpisodeAtLocation,
    EpisodeSawObject,
    EpisodeInvolvedPerson,
    EpisodeRelatedToTask,
    FactDerivedFromEpisode,
    FactAboutObject,
    FactAboutLocation,
    FactAboutPerson,
}

/// An edge in the robot knowledge graph
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphEdge {
    pub from_id: String,
    pub to_id: String,
    pub edge_type: GraphEdgeType,
    pub properties: HashMap<String, serde_json::Value>,
}

// ============================================================================
// ROBOT MEMORY INDEXER
// ============================================================================

/// Configuration for the robot memory indexer
#[derive(Debug, Clone)]
pub struct RobotMemoryIndexerConfig {
    pub kafka_bootstrap_servers: String,
    pub consumer_group_id: String,
    pub episode_topics: Vec<String>,
    pub semantic_topics: Vec<String>,
    pub chunker_service_url: String,
    pub vector_rag_service_url: String,
    pub graph_rag_service_url: String,
    pub batch_size: usize,
    pub batch_timeout_ms: u64,
}

impl Default for RobotMemoryIndexerConfig {
    fn default() -> Self {
        Self {
            kafka_bootstrap_servers: std::env::var("KAFKA_BOOTSTRAP_SERVERS")
                .unwrap_or_else(|_| "localhost:9092".to_string()),
            consumer_group_id: "conhub-robot-memory-indexer".to_string(),
            episode_topics: vec!["robot.*.episodes".to_string()],
            semantic_topics: vec!["robot.*.semantic_events".to_string()],
            chunker_service_url: std::env::var("CHUNKER_SERVICE_URL")
                .unwrap_or_else(|_| "http://localhost:3017".to_string()),
            vector_rag_service_url: std::env::var("VECTOR_RAG_SERVICE_URL")
                .unwrap_or_else(|_| "http://localhost:8082".to_string()),
            graph_rag_service_url: std::env::var("GRAPH_RAG_SERVICE_URL")
                .unwrap_or_else(|_| "http://localhost:8006".to_string()),
            batch_size: 100,
            batch_timeout_ms: 5000,
        }
    }
}

/// Robot memory indexer service
/// 
/// Consumes Kafka topics and indexes robot memory into ConHub's knowledge layer.
pub struct RobotMemoryIndexer {
    config: RobotMemoryIndexerConfig,
    http_client: reqwest::Client,
    /// Relation builder for extracting knowledge graph relations
    relation_builder: crate::relation_builder::RelationBuilder,
    /// In-memory buffer for development (when Kafka is not available)
    episode_buffer: Arc<RwLock<Vec<EpisodeMessage>>>,
    semantic_buffer: Arc<RwLock<Vec<SemanticEventMessage>>>,
    running: Arc<RwLock<bool>>,
}

impl RobotMemoryIndexer {
    /// Create a new robot memory indexer
    pub fn new(config: RobotMemoryIndexerConfig) -> Self {
        let relation_builder = crate::relation_builder::RelationBuilder::new(
            config.graph_rag_service_url.clone()
        );
        
        Self {
            config,
            http_client: reqwest::Client::new(),
            relation_builder,
            episode_buffer: Arc::new(RwLock::new(Vec::new())),
            semantic_buffer: Arc::new(RwLock::new(Vec::new())),
            running: Arc::new(RwLock::new(false)),
        }
    }
    
    /// Create with default configuration
    pub fn from_env() -> Self {
        Self::new(RobotMemoryIndexerConfig::default())
    }
    
    /// Start the indexer (in development mode, processes buffered messages)
    pub async fn start(&self) -> Result<(), IndexerError> {
        let mut running = self.running.write().await;
        if *running {
            return Err(IndexerError::AlreadyRunning);
        }
        *running = true;
        drop(running);
        
        info!("ðŸš€ Robot memory indexer started");
        
        // In production, this would start Kafka consumers
        // For now, we just log that we're ready
        
        let kafka_enabled = std::env::var("KAFKA_ENABLED")
            .map(|v| v.to_lowercase() == "true")
            .unwrap_or(false);
        
        if kafka_enabled {
            info!("ðŸ“¡ Kafka consumers would connect to: {}", self.config.kafka_bootstrap_servers);
            info!("ðŸ“¥ Episode topics: {:?}", self.config.episode_topics);
            info!("ðŸ“¥ Semantic topics: {:?}", self.config.semantic_topics);
            
            // TODO: Start actual Kafka consumers using rdkafka
            // Example:
            // ```
            // use rdkafka::consumer::{StreamConsumer, Consumer};
            // use rdkafka::ClientConfig;
            // 
            // let consumer: StreamConsumer = ClientConfig::new()
            //     .set("group.id", &self.config.consumer_group_id)
            //     .set("bootstrap.servers", &self.config.kafka_bootstrap_servers)
            //     .set("auto.offset.reset", "earliest")
            //     .create()
            //     .expect("Consumer creation failed");
            // 
            // consumer.subscribe(&self.config.episode_topics)?;
            // 
            // loop {
            //     match consumer.recv().await {
            //         Ok(message) => self.process_message(message).await,
            //         Err(e) => error!("Kafka error: {}", e),
            //     }
            // }
            // ```
        } else {
            warn!("âš ï¸ Kafka disabled - indexer running in buffer mode");
        }
        
        Ok(())
    }
    
    /// Stop the indexer
    pub async fn stop(&self) {
        let mut running = self.running.write().await;
        *running = false;
        info!("ðŸ›‘ Robot memory indexer stopped");
    }
    
    /// Process an episode message (can be called directly for testing)
    pub async fn process_episode(&self, episode: EpisodeMessage) -> Result<(), IndexerError> {
        info!("ðŸ“ Processing episode {} from robot {}", 
              episode.episode_number, episode.robot_id);
        
        // Convert to document
        let document = self.episode_to_document(&episode);
        
        // Index into vector store
        self.index_to_vector_store(&document).await?;
        
        // Index into graph store
        self.index_episode_to_graph(&episode).await?;
        
        // Extract relations and build knowledge graph
        let relations = self.relation_builder.process_episode(&episode).await;
        info!("ðŸ”— Extracted {} relations from episode", relations.len());
        
        // Convert relations to semantic facts for storage
        for relation in &relations {
            let fact = self.relation_builder.relation_to_semantic_fact(relation);
            info!("ðŸ“Š Semantic fact: {} {} {}", 
                  fact.subject, fact.predicate, fact.object_value.as_deref().unwrap_or(""));
            // TODO: Store fact in database (robot_semantic_facts table)
        }
        
        info!("âœ… Episode {} indexed successfully", episode.episode_number);
        
        Ok(())
    }
    
    /// Process a semantic event message
    pub async fn process_semantic_event(&self, event: SemanticEventMessage) -> Result<(), IndexerError> {
        info!("ðŸ“ Processing semantic event: {} {} {}", 
              event.subject, event.predicate, event.object_value.as_deref().unwrap_or(""));
        
        // Convert to document
        let document = self.semantic_event_to_document(&event);
        
        // Index into vector store
        self.index_to_vector_store(&document).await?;
        
        // Index into graph store
        self.index_semantic_event_to_graph(&event).await?;
        
        info!("âœ… Semantic event indexed successfully");
        
        Ok(())
    }
    
    /// Convert an episode to a document
    fn episode_to_document(&self, episode: &EpisodeMessage) -> RobotMemoryDocument {
        // Build rich text content for embedding
        let mut content = episode.summary.clone();
        
        if let Some(ref desc) = episode.detailed_description {
            content.push_str("\n\n");
            content.push_str(desc);
        }
        
        if let Some(ref location) = episode.location_name {
            content.push_str(&format!("\n\nLocation: {}", location));
        }
        
        if !episode.objects_seen.is_empty() {
            content.push_str(&format!("\n\nObjects seen: {}", episode.objects_seen.join(", ")));
        }
        
        if !episode.people_involved.is_empty() {
            content.push_str(&format!("\n\nPeople involved: {}", episode.people_involved.join(", ")));
        }
        
        if let Some(ref outcome) = episode.outcome {
            let outcome_text = if outcome.success { "successful" } else { "failed" };
            content.push_str(&format!("\n\nOutcome: {}", outcome_text));
            if let Some(ref error) = outcome.error_message {
                content.push_str(&format!(" - {}", error));
            }
        }
        
        RobotMemoryDocument {
            id: Uuid::new_v4(),
            robot_id: episode.robot_id,
            tenant_id: episode.tenant_id,
            document_type: RobotDocumentType::Episode,
            content,
            metadata: RobotDocumentMetadata {
                robot_id: episode.robot_id.to_string(),
                tenant_id: episode.tenant_id.to_string(),
                document_type: "episode".to_string(),
                episode_number: Some(episode.episode_number),
                episode_type: Some(episode.episode_type.clone()),
                location_id: episode.location_id.clone(),
                location_name: episode.location_name.clone(),
                started_at: Some(episode.started_at.to_rfc3339()),
                ended_at: Some(episode.ended_at.to_rfc3339()),
                objects: episode.objects_seen.clone(),
                people: episode.people_involved.clone(),
                tasks: episode.tasks_related.clone(),
                confidence_score: episode.confidence_score,
                fact_type: None,
                subject: None,
                predicate: None,
            },
            created_at: Utc::now(),
        }
    }
    
    /// Convert a semantic event to a document
    fn semantic_event_to_document(&self, event: &SemanticEventMessage) -> RobotMemoryDocument {
        RobotMemoryDocument {
            id: Uuid::new_v4(),
            robot_id: event.robot_id,
            tenant_id: event.tenant_id,
            document_type: RobotDocumentType::SemanticFact,
            content: event.natural_language.clone(),
            metadata: RobotDocumentMetadata {
                robot_id: event.robot_id.to_string(),
                tenant_id: event.tenant_id.to_string(),
                document_type: "semantic_fact".to_string(),
                episode_number: None,
                episode_type: None,
                location_id: None,
                location_name: None,
                started_at: None,
                ended_at: None,
                objects: vec![],
                people: vec![],
                tasks: vec![],
                confidence_score: Some(event.confidence),
                fact_type: Some(event.event_type.clone()),
                subject: Some(event.subject.clone()),
                predicate: Some(event.predicate.clone()),
            },
            created_at: Utc::now(),
        }
    }
    
    /// Index a document to the vector store
    async fn index_to_vector_store(&self, document: &RobotMemoryDocument) -> Result<(), IndexerError> {
        // TODO: Call chunker service first, then vector_rag service
        // For now, we just log
        
        info!("ðŸ“Š Would index document {} to vector store", document.id);
        info!("   Content length: {} chars", document.content.len());
        info!("   Type: {:?}", document.document_type);
        
        // Example call to chunker:
        // let chunks = self.http_client
        //     .post(&format!("{}/api/chunk", self.config.chunker_service_url))
        //     .json(&ChunkRequest {
        //         content: document.content.clone(),
        //         source_kind: "robot_memory".to_string(),
        //         metadata: document.metadata.clone(),
        //     })
        //     .send()
        //     .await?
        //     .json::<ChunkResponse>()
        //     .await?;
        //
        // for chunk in chunks {
        //     self.http_client
        //         .post(&format!("{}/api/index", self.config.vector_rag_service_url))
        //         .json(&IndexRequest {
        //             collection: "robot_memory".to_string(),
        //             document_id: document.id.to_string(),
        //             chunk_id: chunk.id,
        //             content: chunk.content,
        //             metadata: chunk.metadata,
        //         })
        //         .send()
        //         .await?;
        // }
        
        Ok(())
    }
    
    /// Index an episode to the graph store
    async fn index_episode_to_graph(&self, episode: &EpisodeMessage) -> Result<(), IndexerError> {
        info!("ðŸ”— Would index episode {} to graph store", episode.episode_number);
        
        // Create nodes
        let episode_node = GraphNode {
            id: format!("episode:{}:{}", episode.robot_id, episode.episode_number),
            node_type: GraphNodeType::Episode,
            name: format!("Episode {}", episode.episode_number),
            properties: {
                let mut props = HashMap::new();
                props.insert("summary".to_string(), serde_json::json!(episode.summary));
                props.insert("episode_type".to_string(), serde_json::json!(episode.episode_type));
                props.insert("started_at".to_string(), serde_json::json!(episode.started_at.to_rfc3339()));
                props
            },
        };
        
        info!("   Episode node: {}", episode_node.id);
        
        // Create edges
        let robot_edge = GraphEdge {
            from_id: format!("robot:{}", episode.robot_id),
            to_id: episode_node.id.clone(),
            edge_type: GraphEdgeType::RobotHadEpisode,
            properties: HashMap::new(),
        };
        
        info!("   Robot -> Episode edge");
        
        // Location edges
        if let Some(ref location_id) = episode.location_id {
            let location_edge = GraphEdge {
                from_id: episode_node.id.clone(),
                to_id: format!("location:{}", location_id),
                edge_type: GraphEdgeType::EpisodeAtLocation,
                properties: HashMap::new(),
            };
            info!("   Episode -> Location edge: {}", location_id);
        }
        
        // Object edges
        for object in &episode.objects_seen {
            let object_edge = GraphEdge {
                from_id: episode_node.id.clone(),
                to_id: format!("object:{}", object),
                edge_type: GraphEdgeType::EpisodeSawObject,
                properties: HashMap::new(),
            };
            info!("   Episode -> Object edge: {}", object);
        }
        
        // TODO: Actually call graph_rag service
        // self.http_client
        //     .post(&format!("{}/api/nodes", self.config.graph_rag_service_url))
        //     .json(&episode_node)
        //     .send()
        //     .await?;
        
        Ok(())
    }
    
    /// Index a semantic event to the graph store
    async fn index_semantic_event_to_graph(&self, event: &SemanticEventMessage) -> Result<(), IndexerError> {
        info!("ðŸ”— Would index semantic fact to graph store");
        
        let fact_node = GraphNode {
            id: format!("fact:{}:{}:{}", event.robot_id, event.subject, event.predicate),
            node_type: GraphNodeType::SemanticFact,
            name: event.natural_language.clone(),
            properties: {
                let mut props = HashMap::new();
                props.insert("fact_type".to_string(), serde_json::json!(event.event_type));
                props.insert("subject".to_string(), serde_json::json!(event.subject));
                props.insert("predicate".to_string(), serde_json::json!(event.predicate));
                props.insert("confidence".to_string(), serde_json::json!(event.confidence));
                props
            },
        };
        
        info!("   Fact node: {}", fact_node.id);
        
        Ok(())
    }
    
    /// Add an episode to the buffer (for development/testing)
    pub async fn buffer_episode(&self, episode: EpisodeMessage) {
        let mut buffer = self.episode_buffer.write().await;
        buffer.push(episode);
        info!("ðŸ“¦ Buffered episode (total: {})", buffer.len());
    }
    
    /// Add a semantic event to the buffer (for development/testing)
    pub async fn buffer_semantic_event(&self, event: SemanticEventMessage) {
        let mut buffer = self.semantic_buffer.write().await;
        buffer.push(event);
        info!("ðŸ“¦ Buffered semantic event (total: {})", buffer.len());
    }
    
    /// Process all buffered messages
    pub async fn process_buffer(&self) -> Result<(usize, usize), IndexerError> {
        let episodes: Vec<_> = {
            let mut buffer = self.episode_buffer.write().await;
            std::mem::take(&mut *buffer)
        };
        
        let semantic_events: Vec<_> = {
            let mut buffer = self.semantic_buffer.write().await;
            std::mem::take(&mut *buffer)
        };
        
        let episode_count = episodes.len();
        let semantic_count = semantic_events.len();
        
        for episode in episodes {
            if let Err(e) = self.process_episode(episode).await {
                error!("Failed to process episode: {}", e);
            }
        }
        
        for event in semantic_events {
            if let Err(e) = self.process_semantic_event(event).await {
                error!("Failed to process semantic event: {}", e);
            }
        }
        
        Ok((episode_count, semantic_count))
    }
}

// ============================================================================
// ERROR TYPES
// ============================================================================

#[derive(Debug, thiserror::Error)]
pub enum IndexerError {
    #[error("Indexer already running")]
    AlreadyRunning,
    
    #[error("Kafka error: {0}")]
    KafkaError(String),
    
    #[error("HTTP error: {0}")]
    HttpError(#[from] reqwest::Error),
    
    #[error("Serialization error: {0}")]
    SerializationError(String),
    
    #[error("Indexing error: {0}")]
    IndexingError(String),
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_episode_to_document() {
        let indexer = RobotMemoryIndexer::from_env();
        
        let episode = EpisodeMessage {
            robot_id: Uuid::new_v4(),
            tenant_id: Uuid::new_v4(),
            episode_number: 1,
            episode_type: "navigation".to_string(),
            started_at: Utc::now(),
            ended_at: Utc::now(),
            duration_ms: 5000,
            summary: "Robot navigated from dock A to shelf B3".to_string(),
            detailed_description: Some("The robot successfully navigated through the warehouse".to_string()),
            location_id: Some("warehouse_1".to_string()),
            location_name: Some("Main Warehouse".to_string()),
            location_coordinates: None,
            objects_seen: vec!["pallet".to_string(), "forklift".to_string()],
            people_involved: vec![],
            tasks_related: vec!["delivery_123".to_string()],
            observations_count: 50,
            actions_count: 10,
            outcome: Some(EpisodeOutcome {
                success: true,
                result_type: Some("navigation_complete".to_string()),
                error_message: None,
            }),
            confidence_score: Some(0.95),
        };
        
        let document = indexer.episode_to_document(&episode);
        
        assert_eq!(document.document_type, RobotDocumentType::Episode);
        assert!(document.content.contains("navigated"));
        assert!(document.content.contains("pallet"));
        assert_eq!(document.metadata.episode_number, Some(1));
    }
}
